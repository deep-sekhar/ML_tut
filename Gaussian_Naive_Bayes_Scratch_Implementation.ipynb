{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaussian Naive Bayes - Scratch Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI3WIewuTJAq"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from math import pi\n",
        "from math import e\n",
        "import requests\n",
        "import random\n",
        "import csv\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussNB:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def load_csv(self, data, header=False):\n",
        "        \"\"\"\n",
        "        :param data: raw comma seperated file\n",
        "        :param header: remove header if it exists\n",
        "        :return:\n",
        "        Load and convert each string of data into a float\n",
        "        \"\"\"\n",
        "        lines = csv.reader(data.splitlines())\n",
        "        dataset = list(lines)\n",
        "        if header:\n",
        "            # remove header\n",
        "            dataset = dataset[1:]\n",
        "        for i in range(len(dataset)):\n",
        "            dataset[i] = [float(x) if re.search('\\d', x) else x for x in dataset[i]]\n",
        "        return dataset\n",
        "\n",
        "    def split_data(self, data, weight):\n",
        "        \"\"\"\n",
        "        :param data:\n",
        "        :param weight: indicates the percentage of rows that'll be used for training\n",
        "        :return:\n",
        "        Randomly selects rows for training according to the weight and uses the rest of the rows for testing.\n",
        "        \"\"\"\n",
        "        train_size = int(len(data) * weight)\n",
        "        train_set = []\n",
        "        for i in range(train_size):\n",
        "            index = random.randrange(len(data))\n",
        "            train_set.append(data[index])\n",
        "            data.pop(index)\n",
        "        return [train_set, data]\n",
        "\n",
        "    def group_by_class(self, data, target):\n",
        "        \"\"\"\n",
        "        :param data: Training set. Lists of events (rows) in a list\n",
        "        :param target: Index for the target column. Usually the last index in the list\n",
        "        :return:\n",
        "        Mapping each target to a list of it's features\n",
        "        \"\"\"\n",
        "        target_map = defaultdict(list)\n",
        "        for index in range(len(data)):\n",
        "            features = data[index]\n",
        "            if not features:\n",
        "                continue\n",
        "            x = features[target]\n",
        "            target_map[x].append(features[:-1])  # designating the last column as the class column\n",
        "        return dict(target_map)\n",
        "\n",
        "    def mean(self, numbers):\n",
        "        \"\"\"\n",
        "        :param numbers: list of numbers\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        result = sum(numbers) / float(len(numbers))\n",
        "        return result\n",
        "\n",
        "    def stdev(self, numbers):\n",
        "        \"\"\"\n",
        "        :param numbers: list of numbers\n",
        "        :return:\n",
        "        Calculate the standard deviation for a list of numbers.\n",
        "        \"\"\"\n",
        "        avg = self.mean(numbers)\n",
        "        squared_diff_list = []\n",
        "        for num in numbers:\n",
        "            squared_diff = (num - avg) ** 2\n",
        "            squared_diff_list.append(squared_diff)\n",
        "        squared_diff_sum = sum(squared_diff_list)\n",
        "        sample_n = float(len(numbers) - 1)\n",
        "        var = squared_diff_sum / sample_n\n",
        "        return var ** .5\n",
        "\n",
        "    def summarize(self, test_set):\n",
        "        \"\"\"\n",
        "        :param test_set: lists of features\n",
        "        :return:\n",
        "        Use zip to line up each feature into a single column across multiple lists.\n",
        "        yield the mean and the stdev for each feature.\n",
        "        \"\"\"\n",
        "        for feature in zip(*test_set):\n",
        "            yield {\n",
        "                'stdev': self.stdev(feature),\n",
        "                'mean': self.mean(feature)\n",
        "            }\n",
        "\n",
        "    def prior_prob(self, group, target, data):\n",
        "        \"\"\"\n",
        "        :return:\n",
        "        The probability of each target class\n",
        "        \"\"\"\n",
        "        total = float(len(data))\n",
        "        result = len(group[target]) / total\n",
        "        return result\n",
        "\n",
        "    def train(self, train_list, target):\n",
        "        \"\"\"\n",
        "        :param data:\n",
        "        :param target: target class\n",
        "        :return:\n",
        "        For each target:\n",
        "            1. yield prior_prob: the probability of each class. P(class) eg P(Iris-virginica)\n",
        "            2. yield summary: list of {'mean': 0.0, 'stdev': 0.0}\n",
        "        \"\"\"\n",
        "        group = self.group_by_class(train_list, target)\n",
        "        self.summaries = {}\n",
        "        for target, features in group.items():\n",
        "            self.summaries[target] = {\n",
        "                'prior_prob': self.prior_prob(group, target, train_list),\n",
        "                'summary': [i for i in self.summarize(features)],\n",
        "            }\n",
        "        return self.summaries\n",
        "\n",
        "    def normal_pdf(self, x, mean, stdev):\n",
        "        \"\"\"\n",
        "        :param x: a variable\n",
        "        :param mean: µ - the expected value or average from M samples\n",
        "        :param stdev: σ - standard deviation\n",
        "        :return: Gaussian (Normal) Density function.\n",
        "        N(x; µ, σ) = (1 / 2πσ) * (e ^ (x–µ)^2/-2σ^2\n",
        "        \"\"\"\n",
        "        variance = stdev ** 2\n",
        "        exp_squared_diff = (x - mean) ** 2\n",
        "        exp_power = -exp_squared_diff / (2 * variance)\n",
        "        exponent = e ** exp_power\n",
        "        denominator = ((2 * pi) ** .5) * stdev\n",
        "        normal_prob = exponent / denominator\n",
        "        return normal_prob\n",
        "\n",
        "    def marginal_pdf(self, joint_probabilities):\n",
        "        \"\"\"\n",
        "        :param joint_probabilities: list of joint probabilities for each feature\n",
        "        :return:\n",
        "        Marginal Probability Density Function (Predictor Prior Probability)\n",
        "        Joint Probability = prior * likelihood\n",
        "        Marginal Probability is the sum of all joint probabilities for all classes.\n",
        "        marginal_pdf =\n",
        "          [P(setosa) * P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)]\n",
        "        + [P(versicolour) * P(sepal length | versicolour) * P(sepal width | versicolour) * P(petal length | versicolour) * P(petal width | versicolour)]\n",
        "        + [P(virginica) * P(sepal length | verginica) * P(sepal width | verginica) * P(petal length | verginica) * P(petal width | verginica)]\n",
        "        \"\"\"\n",
        "        marginal_prob = sum(joint_probabilities.values())\n",
        "        return marginal_prob\n",
        "\n",
        "    def joint_probabilities(self, test_row):\n",
        "        \"\"\"\n",
        "        :param test_row: single list of features to test; new data\n",
        "        :return:\n",
        "        Use the normal_pdf(self, x, mean, stdev) to calculate the Normal Probability for each feature\n",
        "        Take the product of all Normal Probabilities and the Prior Probability.\n",
        "        \"\"\"\n",
        "        joint_probs = {}\n",
        "        for target, features in self.summaries.items():\n",
        "            total_features = len(features['summary'])\n",
        "            likelihood = 1\n",
        "            for index in range(total_features):\n",
        "                feature = test_row[index]\n",
        "                mean = features['summary'][index]['mean']\n",
        "                stdev = features['summary'][index]['stdev']\n",
        "                normal_prob = self.normal_pdf(feature, mean, stdev)\n",
        "                likelihood *= normal_prob\n",
        "            prior_prob = features['prior_prob']\n",
        "            joint_probs[target] = prior_prob * likelihood\n",
        "        return joint_probs\n",
        "\n",
        "    def posterior_probabilities(self, test_row):\n",
        "        \"\"\"\n",
        "        :param test_row: single list of features to test; new data\n",
        "        :return:\n",
        "        For each feature (x) in the test_row:\n",
        "            1. Calculate Predictor Prior Probability using the Normal PDF N(x; µ, σ). eg = P(feature | class)\n",
        "            2. Calculate Likelihood by getting the product of the prior and the Normal PDFs\n",
        "            3. Multiply Likelihood by the prior to calculate the Joint Probability.\n",
        "        E.g.\n",
        "        prior_prob: P(setosa)\n",
        "        likelihood: P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)\n",
        "        joint_prob: prior_prob * likelihood\n",
        "        marginal_prob: predictor prior probability\n",
        "        posterior_prob = joint_prob/ marginal_prob\n",
        "        returning a dictionary mapping of class to it's posterior probability\n",
        "        \"\"\"\n",
        "        posterior_probs = {}\n",
        "        joint_probabilities = self.joint_probabilities(test_row)\n",
        "        marginal_prob = self.marginal_pdf(joint_probabilities)\n",
        "        for target, joint_prob in joint_probabilities.items():\n",
        "            posterior_probs[target] = joint_prob / marginal_prob\n",
        "        return posterior_probs\n",
        "\n",
        "    def get_map(self, test_row):\n",
        "        \"\"\"\n",
        "        :param test_row: single list of features to test; new data\n",
        "        :return:\n",
        "        Return the target class with the largest/best posterior probability\n",
        "        \"\"\"\n",
        "        posterior_probs = self.posterior_probabilities(test_row)\n",
        "        map_prob = max(posterior_probs, key=posterior_probs.get)\n",
        "        return map_prob\n",
        "\n",
        "    def predict(self, test_set):\n",
        "        \"\"\"\n",
        "        :param test_set: list of features to test on\n",
        "        :return:\n",
        "        Predict the likeliest target for each row of the test_set.\n",
        "        Return a list of predicted targets.\n",
        "        \"\"\"\n",
        "        map_probs = []\n",
        "        for row in test_set:\n",
        "            map_prob = self.get_map(row)\n",
        "            map_probs.append(map_prob)\n",
        "        return map_probs\n",
        "\n",
        "    def accuracy(self, test_set, predicted):\n",
        "        \"\"\"\n",
        "        :param test_set: list of test_data\n",
        "        :param predicted: list of predicted classes\n",
        "        :return:\n",
        "        Calculate the the average performance of the classifier.\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        actual = [item[-1] for item in test_set]\n",
        "        for x, y in zip(actual, predicted):\n",
        "            if x == y:\n",
        "                correct += 1\n",
        "        return correct / float(len(test_set))"
      ],
      "metadata": {
        "id": "LoTEXnhjTQ7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = GaussNB()\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "data = requests.get(url).content\n",
        "data = nb.load_csv(data.decode('utf-8'), header=True)\n",
        "train_list, test_list = nb.split_data(data, weight=.67)\n",
        "print( \"Using %s rows for training and %s rows for testing\" % (len(train_list), len(test_list)))\n",
        "group = nb.group_by_class(data, -1)  # designating the last column as the class column\n",
        "print (\"Grouped into %s classes: %s\" % (len(group.keys()), group.keys()))\n",
        "nb.train(train_list, -1)\n",
        "predicted = nb.predict(test_list)\n",
        "accuracy = nb.accuracy(test_list, predicted)\n",
        "print ('Accuracy: %.3f' % (accuracy))"
      ],
      "metadata": {
        "id": "H4PwnBJqgC4o",
        "outputId": "6b5cac2e-c06f-4738-dd04-fd271d465196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 100 rows for training and 50 rows for testing\n",
            "Grouped into 3 classes: dict_keys(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1b6e51859c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Grouped into %s classes: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-44511d8dec5e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_set)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mmap_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmap_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mmap_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-44511d8dec5e>\u001b[0m in \u001b[0;36mget_map\u001b[0;34m(self, test_row)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlargest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbest\u001b[0m \u001b[0mposterior\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mposterior_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mmap_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposterior_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposterior_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-44511d8dec5e>\u001b[0m in \u001b[0;36mposterior_probabilities\u001b[0;34m(self, test_row)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m         \u001b[0mposterior_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mjoint_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mmarginal_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarginal_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_prob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoint_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-44511d8dec5e>\u001b[0m in \u001b[0;36mjoint_probabilities\u001b[0;34m(self, test_row)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mstdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stdev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}